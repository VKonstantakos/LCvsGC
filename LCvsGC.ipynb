{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Applied Data Science - Source code.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rfWIwPP0g3U7",
        "4ASo6wyx_V7Z",
        "B8JPcVKMQxOU",
        "QO8nxyFSQ3Er",
        "OojYzf4bir2m",
        "Q7tmetf9D7RI",
        "obTiLgNxi0He",
        "xHJYa_6Ep6Im",
        "MF_aMO4lD3jR",
        "38Nb8DaAkX99",
        "sMweBigMxOiQ",
        "LA2FVK63wwgU",
        "mIGUFGzwgd1f",
        "1waTUid8-S99",
        "3nJLPnI4mfW_",
        "BUBA0o2pi15V",
        "638md90yXkI9",
        "9fZWRvyf4pkc",
        "VomfcRvSkC4T",
        "YIdhLRR3j9Ay",
        "_JLOmotaB6dp",
        "DNFZ3PbUKgRc",
        "sYaNqBZqxpTi"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfWIwPP0g3U7"
      },
      "source": [
        "## Prepare Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zG-zuF6-xzr"
      },
      "source": [
        "Install the necessary requirements (can be run on Google Colab)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd3HVVizXYkR"
      },
      "source": [
        "!pip install -r /content/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaeeJlvy-4k0"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7ER_SU3pkvE"
      },
      "source": [
        "from time import time\n",
        "from collections import Counter\n",
        "\n",
        "import graphviz\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "\n",
        "from joblib import dump, load\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
        "from sklearn.feature_selection import SelectFromModel, VarianceThreshold, RFECV, SequentialFeatureSelector\n",
        "from sklearn.model_selection import GridSearchCV, KFold, RepeatedKFold, cross_validate, train_test_split, validation_curve\n",
        "from sklearn.metrics import multilabel_confusion_matrix, classification_report, roc_curve, auc, accuracy_score, hamming_loss, make_scorer\n",
        "\n",
        "import pingouin as pg\n",
        "import scikit_posthocs as sp\n",
        "from mlxtend.evaluate import mcnemar\n",
        "from mlxtend.evaluate import mcnemar_table\n",
        "\n",
        "sns.set()\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "warnings.simplefilter(\"ignore\", FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ASo6wyx_V7Z"
      },
      "source": [
        "## Define helper functions and pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8JPcVKMQxOU"
      },
      "source": [
        "### Feature selection functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zoaNW9COYpj"
      },
      "source": [
        "def qconstant_filtering(X, thresh=0.01):\n",
        "  '''Remove constant and quasi-constant features.'''\n",
        "\n",
        "  qconstant_filter = VarianceThreshold()\n",
        "  qconstant_filter.fit(X)\n",
        "\n",
        "  qfeatures = X.iloc[:, qconstant_filter.variances_ <= thresh].columns\n",
        "\n",
        "  print(f\"Quasi-Constant features found: {len(qfeatures)}\")\n",
        "\n",
        "  X = X.drop(labels=qfeatures, axis=1)\n",
        "\n",
        "  print(f\"New feature shape: {X.shape}\")\n",
        "  return X\n",
        "\n",
        "\n",
        "def correlation_filtering(X, thresh=0.9, method='spearman'):\n",
        "    '''Remove correlated features.'''\n",
        "\n",
        "    # Set of all the names of deleted columns\n",
        "    col_corr = set()\n",
        "\n",
        "    # Create correlation matrix from the dataset\n",
        "    corr_matrix = X.corr(method=method).abs()\n",
        "\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if (corr_matrix.iloc[i, j] >= thresh) and (corr_matrix.columns[j] not in col_corr):\n",
        "                colname = corr_matrix.columns[i] # getting the name of column\n",
        "                col_corr.add(colname)\n",
        "                if colname in X.columns:\n",
        "                    del X[colname] # deleting the column from the dataset\n",
        "\n",
        "    print(f\"Correlated features found: {len(col_corr)}\")\n",
        "    print(f\"New feature shape: {X.shape}\")\n",
        "    return X\n",
        "\n",
        "def feature_importance(model, X, Y, thresh='1.5*mean'):\n",
        "    '''Select features based on importance weights.'''\n",
        "\n",
        "    # Initialize and apply transformer\n",
        "    forest_sel = SelectFromModel(model, threshold=thresh)\n",
        "\n",
        "    tic = time()\n",
        "\n",
        "    forest_sel.fit(X, Y)\n",
        "\n",
        "    toc = time()\n",
        "\n",
        "    # Transform X feature set\n",
        "    X = X.iloc[:, forest_sel.get_support()]\n",
        "\n",
        "    print(f\"Done in {(toc - tic)/60:0.2f} minutes\")\n",
        "    print(f\"New feature shape: {X.shape}\")\n",
        "    return X\n",
        "\n",
        "def rfecv_selection(model, X, Y, cv=10, scoring=None, figure=False):\n",
        "    '''Feature ranking with recursive feature elimination and\n",
        "    cross-validated selection of the best number of features.'''\n",
        "\n",
        "    # Initialize and apply RFECV\n",
        "    rfecv = RFECV(estimator=model, step=1, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "\n",
        "    tic = time()\n",
        "\n",
        "    rfecv.fit(X, Y)\n",
        "\n",
        "    toc = time()\n",
        "\n",
        "    # Transform X feature set\n",
        "    X = X.iloc[:, rfecv.support_]\n",
        "\n",
        "    print(f\"Done in {(toc - tic)/60:0.2f} minutes\")\n",
        "    print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
        "    print(f\"New feature shape: {X.shape}\")\n",
        "\n",
        "    # Plot RFECV search\n",
        "    if figure == True:\n",
        "\n",
        "        # Plot number of features VS. cross-validation scores\n",
        "        plt.figure(figsize=(16,9))\n",
        "\n",
        "\n",
        "        plt.xticks(np.arange(0, 56, 5).astype(int))\n",
        "        plt.xlabel(\"Number of features selected\", fontsize=15)\n",
        "        plt.ylabel(\"Cross validation score (Accuracy)\", fontsize=15)\n",
        "        plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
        "\n",
        "        plt.savefig('RFECV.png')\n",
        "        plt.show()\n",
        "\n",
        "    return X\n",
        "\n",
        "def forward_selection(model, X, Y, cv=10, n_feat=10, scoring=None, feat_only=False):\n",
        "    '''Perform Sequential Feature Selection in the forward direction.'''\n",
        "\n",
        "    # Initialize and apply Sequential Forward selection\n",
        "    sfs_forward = SequentialFeatureSelector(model, n_features_to_select=n_feat, cv=cv,\n",
        "                                            direction='forward', scoring=scoring, n_jobs=-1)\n",
        "\n",
        "    tic = time()\n",
        "\n",
        "    sfs_forward.fit(X, Y)\n",
        "\n",
        "    toc = time()\n",
        "\n",
        "    # Transform X and get feature set\n",
        "    X = X.iloc[:, sfs_forward.get_support()]\n",
        "    features = X.columns.to_list()\n",
        "\n",
        "    print(f\"Done in {(toc - tic)/60:0.2f} minutes\")\n",
        "\n",
        "    if feat_only == True:\n",
        "        return features\n",
        "\n",
        "    print(f\"New feature shape: {X.shape}\")\n",
        "    print(f\"Features selected by sequential forward selection: {features}\")\n",
        "    return X\n",
        "\n",
        "def manual_selection(features, thresh=0.5):\n",
        "    '''Manually select robust features by calculating overlap.'''\n",
        "\n",
        "    # Calculate feature overlap\n",
        "    count_overlap = Counter(x for xs in features for x in set(xs))\n",
        "\n",
        "    # Initialize list to collect features\n",
        "    sel_feat = []\n",
        "    start = len(features)\n",
        "    stop = int(len(features)*thresh)\n",
        "\n",
        "\n",
        "    for i in range(start, stop, -1):\n",
        "        for k, v in count_overlap.items():\n",
        "            if v == i and len(sel_feat) < 10:\n",
        "                sel_feat.append(k)\n",
        "\n",
        "    print(f'Selected features with good overlap: {sel_feat}')\n",
        "\n",
        "    return sel_feat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO8nxyFSQ3Er"
      },
      "source": [
        "### Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktbcbnH-Q1ps"
      },
      "source": [
        "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
        "    '''Compute the Hamming score (i.e., label-based accuracy) for the multi-label case.'''\n",
        "\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set( np.where(y_true[i])[0] )\n",
        "        set_pred = set( np.where(y_pred[i])[0] )\n",
        "        tmp_a = None\n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
        "                    float( len(set_true.union(set_pred)) )\n",
        "        acc_list.append(tmp_a)\n",
        "    return np.mean(acc_list)\n",
        "\n",
        "def model_comparison(models, X, Y, cv=10, scoring=None):\n",
        "    '''Model comparison using dictionary with different pipelines.'''\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    names = []\n",
        "    results = []\n",
        "\n",
        "    # k-fold CV results\n",
        "    for key, value in models.items():\n",
        "        cv_results = cross_validate(value['pl'], X, Y, cv=cv, scoring = scoring, n_jobs=4)\n",
        "        results.append(cv_results)\n",
        "        names.append(value['name'])\n",
        "\n",
        "        if scoring:\n",
        "            print(f\"\\n{value['name']:<11}: Exact Match Ratio =    {cv_results['test_Exact Match Ratio'].mean():0.3f} ({cv_results['test_Exact Match Ratio'].std():0.3f})\")\n",
        "            print(f\"{' ':<11}  Label-based Accuracy = {cv_results['test_Label-based Accuracy'].mean():0.3f} ({cv_results['test_Label-based Accuracy'].std():0.3f})\")\n",
        "        else:\n",
        "            print(f\"{value['name']:<11}: {cv_results['test_score'].mean():0.3f} ({cv_results['test_score'].std():0.3f})\")\n",
        "\n",
        "    return results, names\n",
        "\n",
        "def feature_comparison(model, X, Y, cv=10, scoring=None, X_names=None, save=True):\n",
        "    '''Perform cross-validation using the same classifier with different feature sets.'''\n",
        "\n",
        "    # Initialize dataframe to store results\n",
        "    ds_result = pd.DataFrame()\n",
        "\n",
        "    # Perform cross-validation and save the results\n",
        "    if X_names:\n",
        "        for name, data in zip(X_names, X):\n",
        "            cv_results = cross_validate(model, data, Y, cv=cv, scoring=scoring, n_jobs=4)\n",
        "            ds_result[name] = cv_results['test_score']\n",
        "    else:\n",
        "        for idx, data in enumerate(X):\n",
        "            cv_results = cross_validate(model, data, Y, cv=cv, scoring=scoring, n_jobs=4)\n",
        "            ds_result[idx] = cv_results['test_score']\n",
        "\n",
        "    if save == True:\n",
        "        ds_result.to_csv('Performance of different feature sets in CV.csv', index=None)\n",
        "\n",
        "    return ds_result\n",
        "\n",
        "def accuracy_per_class(y_true, y_pred):\n",
        "    '''Calculate accuracy per class.'''\n",
        "\n",
        "    # Compute confusion matrix for the multilabel case\n",
        "    cm = multilabel_confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Calculate accuracy for each label (TP + TN)/(TP + FP + TN + FN)\n",
        "    gc_acc = cm[0].diagonal().sum()/cm[0].sum()\n",
        "    lc_acc = cm[1].diagonal().sum()/cm[1].sum()\n",
        "\n",
        "    return (gc_acc, lc_acc)\n",
        "\n",
        "def general_classification_report(y_true, y_pred, title='Classification Report'):\n",
        "    '''Show a Classification Report with accuracy.'''\n",
        "\n",
        "    print(title)\n",
        "    print('-'*len(title))\n",
        "    print(classification_report(y_true, y_pred, zero_division=0))\n",
        "\n",
        "    print(\"\\nAccuracy per class\")\n",
        "    print(f'GC = {accuracy_per_class(y_true, y_pred)[0]:0.3f}')\n",
        "    print(f'LC = {accuracy_per_class(y_true, y_pred)[1]:0.3f}')\n",
        "\n",
        "    print(f'\\nLabel-based accuracy = {hamming_score(y_true, y_pred):0.3f}')\n",
        "\n",
        "    return classification_report(y_true, y_pred, zero_division=0, output_dict=True)\n",
        "\n",
        "def show_confusion_matrix(confusion_matrix, axes, class_label, class_names, fontsize=14):\n",
        "    '''Calculate and plot confusion matrix.'''\n",
        "\n",
        "    # Plot heatmap from confusion matrix\n",
        "    ds_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names)\n",
        "\n",
        "    try:\n",
        "        heatmap = sns.heatmap(ds_cm, annot=True, fmt=\"d\", cbar=False, ax=axes, cmap='Blues')\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
        "\n",
        "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
        "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
        "    axes.set_ylabel('True label')\n",
        "    axes.set_xlabel('Predicted label')\n",
        "    axes.set_title(\"Confusion Matrix for the \" + class_label + \" class\", fontsize=14)\n",
        "\n",
        "def plot_roc_curve(y_true, y_pred1, y_pred2, model_names, class_names=None, title='ROC curve', save=False):\n",
        "    '''Calculate and plot ROC curve.'''\n",
        "\n",
        "    # Get number of classes/labels\n",
        "    n_classes = y_true.shape[1]\n",
        "\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr, tpr, roc_auc = dict(), dict(), dict()\n",
        "\n",
        "    for i in range(0, n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred1[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    for i in range(n_classes, 2*n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i-n_classes], y_pred2[:, i-n_classes])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    if class_names:\n",
        "        for i in range(0, 2*n_classes):\n",
        "          plt.plot(fpr[i], tpr[i], lw=2,\n",
        "                    label=f'{model_names[i]}: {class_names[i%2]} class (AUC = {roc_auc[i]:0.3f})')\n",
        "    else:\n",
        "        for i in range(2*n_classes):\n",
        "            plt.plot(fpr[i], tpr[i], lw=2,\n",
        "                     label=f'{model_names[i]}: Class {i%2} (AUC = {roc_auc[i]:0.3f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(title)\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(title + '.png')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OojYzf4bir2m"
      },
      "source": [
        "### Optimization functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71u8jB1Sior0"
      },
      "source": [
        "def tree_optimization(model, param_grid, X, Y, cv=10, scoring=None):\n",
        "    '''Optimize Decision Tree Classifier using Grid Search.'''\n",
        "\n",
        "    # Initialize and apply GridSearchCV\n",
        "    grid_cv = GridSearchCV(model, param_grid, cv=cv, verbose=1, n_jobs=-1, scoring=scoring)\n",
        "\n",
        "    tic = time()\n",
        "\n",
        "    grid_cv.fit(X, Y)\n",
        "\n",
        "    toc = time()\n",
        "\n",
        "    print(f\"Done in {(toc - tic)/60:0.2f} minutes\")\n",
        "    print(f\"Best estimator: {grid_cv.best_estimator_} with score {grid_cv.best_score_}\")\n",
        "\n",
        "    return grid_cv.best_estimator_\n",
        "\n",
        "def plot_validation_curve(estimator, X, y, param_name, param_range = np.arange(1, 30), ylim=(0.0, 1.1), cv=10, scoring='accuracy', save=False):\n",
        "    '''Generate a plot of the validation curve for a given hyperparameter.'''\n",
        "\n",
        "    # Calculate scores\n",
        "    train_scores, test_scores = validation_curve(\n",
        "                                estimator, X, y, param_name=param_name, cv=cv, \n",
        "                                param_range=param_range,\n",
        "                                scoring=scoring)\n",
        "\n",
        "    train_scores_mean = np.mean(train_scores, axis = 1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis = 1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    # Plotting options\n",
        "    plt.figure(figsize=(18, 9))\n",
        "    plt.xlabel(param_name, fontsize=15)\n",
        "    plt.xticks(param_range, fontsize=13)\n",
        "    plt.yticks(np.arange(0, 1.1, 0.1), fontsize=13)\n",
        "    plt.ylabel(\"Hamming Score\", fontsize=15)\n",
        "    plt.ylim(*ylim)\n",
        "\n",
        "    plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
        "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
        "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
        "                 color=\"darkorange\", lw=2)\n",
        "    plt.plot(param_range, test_scores_mean, label=\"Test score\", color=\"g\")\n",
        "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
        "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
        "                 color=\"navy\", lw=2)\n",
        "    plt.legend(loc=\"best\", fontsize=15)\n",
        "\n",
        "    if save == True:\n",
        "        plt.savefig('Decision Tree Validation Curve (max_depth).png')\n",
        "\n",
        "    return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7tmetf9D7RI"
      },
      "source": [
        "### Statistical functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCAC3v19D7Xw"
      },
      "source": [
        "def friedman_test(dataset, alpha=0.05, long=False, effect_size=True):\n",
        "    '''Perform the Friedman test on a provided dataset.'''\n",
        "\n",
        "    # Transform dataframe to the appropriate format\n",
        "    if not long:\n",
        "        dataset = dataset.melt(ignore_index=False).reset_index(drop=False)\n",
        "        dataset = dataset.rename(columns={'index': 'Fold', 'variable': 'Feature set', 'value': 'Accuracy'})\n",
        "\n",
        "    # Perform the Friedman test\n",
        "    result = pg.friedman(dataset, dv='Accuracy', within='Fold', subject='Feature set')\n",
        "\n",
        "    fried, p_value = result['Q'][0], result['p-unc'][0]\n",
        "\n",
        "    print(f'Friedman statistic: {fried:0.2f}')\n",
        "    print(f'p-value: {p_value:0.6f}')\n",
        "\n",
        "    if p_value < alpha:\n",
        "        print('\\nThe differences between some of the groups are statistically significant.')\n",
        "        print('Use a post hoc test to determine which classifiers actually differ.')\n",
        "    else:\n",
        "        print('\\nThe differences between the groups are not statistically significant.')\n",
        "\n",
        "    # Calculate effect size\n",
        "    if effect_size == True:\n",
        "        print('\\nRelevant effect size')\n",
        "        print('----------------------')\n",
        "        print(f\"Kendall's W coefficient: {result['W'][0]:0.3f}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def nemeyi_test(dataset, alpha=0.05, significant_only=False):\n",
        "    '''Perform the Nemeyi Post Hoc test on a provided dataset.'''\n",
        "\n",
        "    # Perform the Nemenyi test\n",
        "    ds_nemeyi = sp.posthoc_nemenyi_friedman(dataset)\n",
        "\n",
        "    if significant_only == True:\n",
        "        print(f'Returning only statistically significant differences (p value < {alpha})')\n",
        "\n",
        "        return ds_nemeyi[ds_nemeyi < alpha]\n",
        "\n",
        "    return ds_nemeyi\n",
        "\n",
        "def wilcoxon_test(x, y, alpha=0.05, alternative='two-sided', effect_size=True):\n",
        "    '''Perform the Wilcoxon-Signed Rank test on two sets of observations.'''\n",
        "\n",
        "    # Perform the Wilcoxon Test\n",
        "    result = pg.wilcoxon(x, y, tail=alternative)\n",
        "\n",
        "    wilcoxon, p_value = result['W-val'][0], result['p-val'][0]\n",
        "\n",
        "    print(f'Wilcoxon statistic: {wilcoxon:0.2f}')\n",
        "    print(f'p-value: {p_value:0.6f}')\n",
        "\n",
        "    if p_value < alpha:\n",
        "        print('\\nThe differences between measurements are statistically significant.')\n",
        "    else:\n",
        "        print('\\nThe differences between measurements are not statistically significant.')\n",
        "\n",
        "    # Calculate effect size\n",
        "    if effect_size == True:\n",
        "        print('\\nRelevant effect sizes')\n",
        "        print('-----------------------')\n",
        "        print(f\"Rank-biserial correlation: {result['RBC'][0]:0.3f}\")\n",
        "        print(f\"Common language effect size: {result['CLES'][0]:0.3f}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def mcnemar_test(y_true, y_pred1, y_pred2, visualize=True, save=True, title=\"McNemar's table\", model_names=None, effect_size=True):\n",
        "    '''Calculate McNemar's test to compare two classifiers.'''\n",
        "\n",
        "    # Compute 2x2 contingency table\n",
        "    table = mcnemar_table(y_true, y_pred1, y_pred2)\n",
        "\n",
        "    # Plot 2x2 contingency table\n",
        "    if visualize == True:\n",
        "        if model_names:\n",
        "            col_labels=['Rule-based Classifier Correct', 'Rule-based Classifier Incorrect']\n",
        "            row_labels=['Decision Tree Classifier Correct', 'Decision Tree Classifier Incorrect']\n",
        "        else:\n",
        "            col_labels=['Model 1 Correct', 'Model 1 Incorrect']\n",
        "            row_labels=['Model 2 Correct', 'Model 2 Incorrect']\n",
        "\n",
        "        plt.figure(figsize=(12, 9))\n",
        "\n",
        "        ds_table = pd.DataFrame(table, index=row_labels, columns=col_labels)\n",
        "\n",
        "        heatmap = sns.heatmap(ds_table, annot=True, fmt=\"d\", cbar=False, cmap='Blues')\n",
        "\n",
        "        plt.yticks([0.2, 1.15])\n",
        "\n",
        "        plt.title(title)\n",
        "\n",
        "        if save == True:\n",
        "            plt.savefig(title + '.png')\n",
        "\n",
        "    # Calculate test statistic and corresponding p-value\n",
        "    chi2, p = mcnemar(table)\n",
        "\n",
        "    print(f'Chi-squared statistic: {chi2:0.2f}')\n",
        "    print(f'p-value: {p:0.6f}')\n",
        "\n",
        "    # Calculate effect size\n",
        "    if effect_size == True:\n",
        "\n",
        "        # Calculate odds ratio (OR)\n",
        "        b = table[0, 1]\n",
        "        c = table[1, 0]\n",
        "        OR = max(b/c, c/b)\n",
        "\n",
        "        print('\\nRelevant effect size')\n",
        "        print('----------------------')\n",
        "        print(f'Odds Ratio: {OR:0.2f}')\n",
        "\n",
        "        return pd.DataFrame({'chi2': chi2, 'p-val': p, 'OR': OR}, index=['McNemar'])\n",
        "\n",
        "    return pd.DataFrame({'chi2': chi2, 'p-val': p}, index=['McNemar'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obTiLgNxi0He"
      },
      "source": [
        "### Baseline classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woboB3iP5EWP"
      },
      "source": [
        "class RulePredict:\n",
        "\n",
        "\n",
        "    def __init__(self, multilabel=True, GC=None, LC=None):\n",
        "        '''Initialize Rule-based Predictor.'''\n",
        "\n",
        "        self.GC = GC\n",
        "        self.LC = LC\n",
        "        self.multilabel = multilabel\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''Predict GC-LC class for the input X.'''\n",
        "\n",
        "        if self.multilabel == True:\n",
        "            return np.array([self.gc_predict(X), self.lc_predict(X)])\n",
        "\n",
        "        elif self.GC == True:\n",
        "            return self.gc_predict(X)\n",
        "\n",
        "        elif self.LC == True:\n",
        "            return self.lc_predict(X)\n",
        "\n",
        "        else:\n",
        "            raise ValueError('Invalid input. Choose a valid method (multilabel, GC, LC).')\n",
        "\n",
        "    def gc_predict_proba(self, X):\n",
        "        '''Predict GC class probabilities of the input samples X.'''\n",
        "\n",
        "        prob = 0\n",
        "\n",
        "        if 100 <= X['BoilingPoint'] <= 350:\n",
        "            prob += 1/3\n",
        "        if 2 <= X['logP']:\n",
        "            prob += 1/3\n",
        "        if X['MW'] <= 700:\n",
        "            prob += 1/3\n",
        "\n",
        "        return prob\n",
        "\n",
        "    def gc_predict(self, X, threshold=0.5):\n",
        "        '''Predict GC class value for X.'''\n",
        "\n",
        "\n",
        "        prob = self.gc_predict_proba(X)\n",
        "\n",
        "        return 1 if prob >= threshold else 0\n",
        "\n",
        "    def lc_predict_proba(self, X):\n",
        "        '''Predict LC class probabilities of the input samples X.'''\n",
        "\n",
        "        prob = 0\n",
        "\n",
        "        if X['logP'] <= 5.91:\n",
        "            prob += 1\n",
        "\n",
        "        return prob\n",
        "\n",
        "    def lc_predict(self, X, threshold=0.5):\n",
        "        '''Predict LC class value for X.'''\n",
        "\n",
        "        prob = self.lc_predict_proba(X)\n",
        "\n",
        "        return 1 if prob >= threshold else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6Zj1oB6_WBq"
      },
      "source": [
        "model_dict = \\\n",
        "    {\n",
        "    # experiments to build pipeline\n",
        "    # Note: keys are of the form model_*, which are used to execute the associated values of 'pl' keys\n",
        "    'model_1': {\n",
        "        'name': 'Dummy',\n",
        "        'pl': Pipeline([ ('dummy_clf', DummyClassifier(strategy='stratified', random_state=42)) ])\n",
        "        },\n",
        "    # systematic check of default classifiers + scaling\n",
        "    'model_2': {\n",
        "        'name': 'KNN',\n",
        "        'pl': Pipeline([ ('knn', KNeighborsClassifier(n_jobs=-1)) ])\n",
        "        },\n",
        "    'model_3': {\n",
        "        'name': 'Scaled KNN',\n",
        "        'pl': Pipeline([ ('scaling', StandardScaler()),\n",
        "                        ('knn', KNeighborsClassifier(n_jobs=-1)) ])\n",
        "        },\n",
        "    'model_4': {\n",
        "        'name': 'DT',\n",
        "        'pl': Pipeline([ ('decision-tree', DecisionTreeClassifier(random_state=42)) ])\n",
        "        },\n",
        "    'model_5': {\n",
        "        'name': 'Scaled DT',\n",
        "        'pl': Pipeline([ ('scaling', StandardScaler()),\n",
        "                        ('decision-tree', DecisionTreeClassifier(random_state=42)) ])\n",
        "        },\n",
        "    'model_6': {\n",
        "        'name': 'RF',\n",
        "        'pl': Pipeline([ ('random-forest', RandomForestClassifier(random_state=42, n_estimators=100)) ])\n",
        "        },\n",
        "    'model_7': {\n",
        "        'name': 'Scaled RF',\n",
        "        'pl': Pipeline([ ('scaling', StandardScaler()),\n",
        "                        ('random-forest', RandomForestClassifier(random_state=42, n_estimators=100)) ])\n",
        "        }\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHJYa_6Ep6Im"
      },
      "source": [
        "## Load and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3FD23USagI7"
      },
      "source": [
        "path = 'http://users.uoa.gr/~nalygizakis/Applied%20Data%20Science/GC_LC%20dataset.xlsx'\n",
        "ds = pd.read_excel(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4xV025d7w7N"
      },
      "source": [
        "ds.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05gkHmrfkaIy"
      },
      "source": [
        "Check for null values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS9ZF-uf9vxC"
      },
      "source": [
        "print(f'Null values per feature: \\n\\n{ds.iloc[:, 7:].isna().sum()}')\n",
        "print(f'\\nTotal null values: {ds.iloc[:, 7:].isna().sum().sum()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33lUM7xkkaTs"
      },
      "source": [
        "Check for duplicated entries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v2mkkt6kacM"
      },
      "source": [
        "print(f'Total duplicated entries: {int(ds[ds.duplicated()].sum().sum())}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVo2savVkgr_"
      },
      "source": [
        "Define feature set and target variable for multilabel problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hfJihp6WwPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83ae3cd0-b12b-45ea-be7f-03022e5b211f"
      },
      "source": [
        "# Initial feature set\n",
        "X = ds.iloc[:, 9:]\n",
        "\n",
        "# Target variable (multilabel)\n",
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit([['GC', 'LC']])\n",
        "y = mlb.transform(ds['Response'].str.split(';'))\n",
        "\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6431, 1446) (6431, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdJP9k6aTqyR"
      },
      "source": [
        "Check data types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0gdpoFrVZBd"
      },
      "source": [
        "X.dtypes.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk1M1NXe0m_q"
      },
      "source": [
        "To simulate a realistic application we will split into Train and Test set and ignore completely the Test set from the start. We will only use it after completing the creation process to evaluate our model's generalization. \n",
        "\n",
        "This way, the Test set will be an unbiased estimator of the true error (the test error will approximate the true generalization error)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNqyP7Ha5g0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9308289f-c05e-430d-b07d-0bbfaf705060"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f'X_train = {X_train.shape}, Y_train = {Y_train.shape}, X_test = {X_test.shape}, Y_test = {Y_test.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train = (5144, 1446), Y_train = (5144, 2), X_test = (1287, 1446), Y_test = (1287, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF_aMO4lD3jR"
      },
      "source": [
        "## Feature selection\n",
        "\n",
        "Apply various feature selection methods to find an accurate and interpretable feature set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Nb8DaAkX99"
      },
      "source": [
        "### Filter methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09YzOQdap2JU"
      },
      "source": [
        "Remove constant/quasi-constant features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UYNS2CysaIy"
      },
      "source": [
        "X_vt = qconstant_filtering(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQHjeb80NBL-"
      },
      "source": [
        "Correlation feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iMI7vQLEYMV"
      },
      "source": [
        "X_corr = X_vt.copy()\n",
        "X_corr = correlation_filtering(X_corr) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMweBigMxOiQ"
      },
      "source": [
        "### Random Forest feature importances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGrbNQUaHsFR"
      },
      "source": [
        "Select features based on random forest importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzM7v_y9VAL7"
      },
      "source": [
        "model = RandomForestClassifier(n_estimators = 100, random_state=42)\n",
        "\n",
        "X_forest = feature_importance(model, X_corr, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA2FVK63wwgU"
      },
      "source": [
        "### RFECV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vHDPlN8HwhM"
      },
      "source": [
        "Find the optimal number of features using RFECV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAH2bsWCe68N"
      },
      "source": [
        "# Model and CV split\n",
        "k_fold = KFold(n_splits=10, shuffle = True, random_state = 200)\n",
        "model = RandomForestClassifier(n_estimators = 100, random_state=200)\n",
        "\n",
        "# Scoring (label-based accuracy)\n",
        "lab_accu = make_scorer(hamming_score)\n",
        "\n",
        "X_rfe = rfecv_selection(model, X_forest, Y_train, cv=k_fold, scoring=lab_accu, figure=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIGUFGzwgd1f"
      },
      "source": [
        "### Sequential feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMgrIFNUHyvd"
      },
      "source": [
        "Calculate optimal feature subsets using sequential forward selection.\n",
        "\n",
        "**Warning**: the following cell may take a very long time (possibly 6-7 hours depending on your hardware)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnFdWOp8z_qn"
      },
      "source": [
        "# Define list for results\n",
        "feat_10 = []\n",
        "\n",
        "# Scoring (label-based accuracy)\n",
        "lab_accu = make_scorer(hamming_score)\n",
        "\n",
        "for i in range(0, 5):bet\n",
        "    k_fold = KFold(n_splits=10, shuffle = True, random_state = i)\n",
        "    model = RandomForestClassifier(n_estimators = 100, random_state = i)\n",
        "\n",
        "    feat_10.append(forward_selection(model, X_forest, Y_train, cv=k_fold, scoring=lab_accu, feat_only=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1waTUid8-S99"
      },
      "source": [
        "### Manual feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee6pSR2lGnmd"
      },
      "source": [
        "sel_feat = manual_selection(feat_10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBI3oPlDB3U_"
      },
      "source": [
        "Present the final selected features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zlHn5qNi1_N"
      },
      "source": [
        "# 1. Minimum E-State\n",
        "# 2. Topological polar surface area\n",
        "# 3. Boiling point\n",
        "# 4. nhigh lowest partial charge weighted BCUTS\n",
        "# 5. Number of nitrogen atoms\n",
        "# 6. Number of basic groups\n",
        "# 7. Overall or summation solute hydrogen bond acidity\n",
        "# 8. Maximum H E-State\n",
        "\n",
        "sel_feat = ['gmin', 'TopoPSA', 'BoilingPoint', 'BCUTc-1l', 'nN', 'nBase', 'MLFER_A', 'hmax']\n",
        "\n",
        "# Transform X\n",
        "X_sel = X_train[sel_feat]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nJLPnI4mfW_"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "Perform model comparison in different feature sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkhe2J9OJ6XR"
      },
      "source": [
        "kfold = KFold(n_splits=10, shuffle = True, random_state = 42)\n",
        "\n",
        "scoring = {'Exact Match Ratio': make_scorer(accuracy_score),\n",
        "           'Label-based Accuracy': make_scorer(hamming_score)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiOvIhY1J_5t"
      },
      "source": [
        "results, names = model_comparison(model_dict, X_train, Y_train, cv=kfold, scoring=scoring)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUBA0o2pi15V"
      },
      "source": [
        "## Statistical comparison of feature sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lE7bQGPGco7"
      },
      "source": [
        "Obtain the accuracy results of the same classifier trained in different feature sets.\n",
        "\n",
        "The evaluation is done in a 10x10 cross-validation schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAJ0BJqNzjRg"
      },
      "source": [
        "# Define classifier and cross-validation schema\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "kfold = RepeatedKFold(n_splits=10, n_repeats=10, random_state = 42)\n",
        "\n",
        "# Scoring (label-based accuracy)\n",
        "lab_accu = make_scorer(hamming_score)\n",
        "\n",
        "# Feature sets to test\n",
        "datasets = [X_train, X_vt, X_corr, X_forest, X_rfe, X_sel]\n",
        "names = ['Initial', 'Constant Filtering', 'Correlation Filtering', 'RF Feature Importance', 'RFECV', 'Final']\n",
        "\n",
        "# Perform cross-validation\n",
        "ds_result = feature_comparison(dt, datasets, Y_train, cv=kfold, scoring=lab_accu, X_names=names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrFftc_5G1Ae"
      },
      "source": [
        "Apply the Friedman test in the calculated results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-3AC42DGKa8"
      },
      "source": [
        "# Calculate Friedman test\n",
        "ds_friedman = friedman_test(ds_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YhZlKDOHAGA"
      },
      "source": [
        "Since the Friedman test showed statistically significant differences, apply the Nemeyi post hoc test to determine the specific differences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0cbO7uZHAMP"
      },
      "source": [
        "# Calculate Nemeyi test\n",
        "ds_nemeyi = nemeyi_test(ds_result)\n",
        "\n",
        "# Show p values for each pair\n",
        "ds_nemeyi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLI9DaUEHNZf"
      },
      "source": [
        "Finally, compare the initial (1446 features) and final (8 features) sets using Wilcoxon signed rank test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbu7-62_HNg8"
      },
      "source": [
        "# Get the relevant groups\n",
        "init_perf = ds_result['Initial']\n",
        "fin_perf = ds_result['Final']\n",
        "\n",
        "# Calculate Wilcoxon test\n",
        "wilc_res = wilcoxon_test(fin_perf, init_perf, alternative='one-sided')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "638md90yXkI9"
      },
      "source": [
        "## Hyperparameter Optimization\n",
        "\n",
        "Optimize Decision Tree classifier on final transformed dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3NQ597UOPLY"
      },
      "source": [
        "# Transform X\n",
        "X_sel = X_train[sel_feat]\n",
        "\n",
        "# Decision Tree Classifier\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Scoring (label-based accuracy)\n",
        "lab_accu = make_scorer(hamming_score)\n",
        "\n",
        "# Cross-validation split\n",
        "gs_kfold = KFold(n_splits=10, shuffle = True, random_state = 42)\n",
        "\n",
        "# Parameter grid for Decision Tree Classifier\n",
        "param_grid = {\"criterion\": [\"gini\", \"entropy\"],\n",
        "              \"max_features\": [\"auto\", \"sqrt\", \"None\"],\n",
        "              \"min_samples_split\": [2, 3, 5, 8, 10, 20, 40],\n",
        "              \"max_depth\": range(3, 30),\n",
        "              \"min_samples_leaf\": [1, 2, 3, 4, 5, 10, 20, 40]\n",
        "              }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaxD6oGwO23t"
      },
      "source": [
        "opt_tree = tree_optimization(tree, param_grid, X_sel, Y_train, cv=gs_kfold, scoring=lab_accu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-rdeeC0c5B9"
      },
      "source": [
        "For label-based accuracy = 0.813, the best estimator is a Decision Tree Classifier with max_depth=26 and max_features='auto'.\n",
        "\n",
        "Therefore, we need to find the optimal depth for interpretability and performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps37V5lqg7Jh"
      },
      "source": [
        "# Decision Tree Classifier\n",
        "dtree = DecisionTreeClassifier(max_features='auto', random_state=42)\n",
        "\n",
        "# Range of depth\n",
        "depths = list(range(1, 31))\n",
        "\n",
        "# Scoring (label-based accuracy)\n",
        "lab_accu = make_scorer(hamming_score)\n",
        "\n",
        "plot_validation_curve(dtree, X_sel, Y_train, 'max_depth', param_range=depths,\n",
        "                      cv=gs_kfold, scoring=lab_accu, ylim=(0.7, 1.01), save=True)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fZWRvyf4pkc"
      },
      "source": [
        "## Train classifier and evaluate on test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VomfcRvSkC4T"
      },
      "source": [
        "### Rule-based classifier - Baseline results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FvOJIc9B0Vs"
      },
      "source": [
        "Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-XyeA82FlBg"
      },
      "source": [
        "# Initialize Rule-based Classifier and predict on test set\n",
        "rule_classifier = RulePredict()\n",
        "rule_pred = np.vstack(X_test.apply(rule_classifier.predict, axis=1))\n",
        "\n",
        "title = 'Classification Report - Rule-based Classifier'\n",
        "\n",
        "# Evaluate\n",
        "rule_report = general_classification_report(Y_test, rule_pred, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrE4tR-XB4h0"
      },
      "source": [
        "Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVFvSxbVh4TX"
      },
      "source": [
        "cm = multilabel_confusion_matrix(Y_test, rule_pred)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 7))\n",
        "\n",
        "fig.suptitle('Confusion Matrix - Rule-based Classifier', y=1.05)\n",
        "\n",
        "show_confusion_matrix(cm[0], ax[0], 'GC', ['N', 'Y'])\n",
        "show_confusion_matrix(cm[1], ax[1], 'LC', ['N', 'Y'])\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIdhLRR3j9Ay"
      },
      "source": [
        "### Decision Tree results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL1p9sGVC75d"
      },
      "source": [
        "Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yz-mJg1hG-k"
      },
      "source": [
        "# Initialize Decision Tree\n",
        "dtree = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "\n",
        "# Transform Train/Test set using selected features\n",
        "sel_feat = ['gmin', 'TopoPSA', 'BoilingPoint', 'BCUTc-1l', 'nN', 'nBase', 'MLFER_A', 'hmax']\n",
        "X_sel = X_train[sel_feat]\n",
        "X_test_sel = X_test[sel_feat]\n",
        "\n",
        "# Train on final dataset\n",
        "dtree.fit(X_sel, Y_train)\n",
        "\n",
        "# Predict on test set\n",
        "tree_pred = dtree.predict(X_test_sel)\n",
        "\n",
        "# Save model\n",
        "dump(dtree, 'DecisionTree.joblib')\n",
        "\n",
        "title = 'Classification Report - Decision Tree Classifier'\n",
        "\n",
        "# Evaluate\n",
        "tree_report = general_classification_report(Y_test, tree_pred, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGhddya-C9Q6"
      },
      "source": [
        "Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlIgAt44XGSN"
      },
      "source": [
        "cm = multilabel_confusion_matrix(Y_test, tree_pred)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 7))\n",
        "\n",
        "fig.suptitle('Confusion Matrix - Decision Tree Classifier', y=1.05)\n",
        "\n",
        "show_confusion_matrix(cm[0], ax[0], 'GC', ['N', 'Y'])\n",
        "show_confusion_matrix(cm[1], ax[1], 'LC', ['N', 'Y'])\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JLOmotaB6dp"
      },
      "source": [
        "### ROC Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZiMrHbzBr3x"
      },
      "source": [
        "classes = ['GC', 'LC']\n",
        "models = ['Rule-based Classifier', 'Rule-based Classifier', 'Decision Tree Classifier', 'Decision Tree Classifier']\n",
        "\n",
        "plot_roc_curve(Y_test, rule_pred, tree_pred, models, class_names=classes, save=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNFZ3PbUKgRc"
      },
      "source": [
        "### Compare classifiers using McNemar's test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0alV8owffND"
      },
      "source": [
        "Calculate test for GC class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTM9p8zKT0qf"
      },
      "source": [
        "# Get GC predictions\n",
        "gc_true = Y_test[:, 0]\n",
        "gc_rule_pred = rule_pred[:, 0]\n",
        "gc_tree_pred = tree_pred[:, 0]\n",
        "\n",
        "models = ['Rule-based Classifier', 'Decision Tree Classifier']\n",
        "title = \"McNemar's table for GC class\"\n",
        "\n",
        "# Compute test\n",
        "gc_mcnemar = mcnemar_test(gc_true, gc_rule_pred, gc_tree_pred, model_names=models, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuxcRI1Ufjh0"
      },
      "source": [
        "Calculate test for LC class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6pjG0tJfjnF"
      },
      "source": [
        "# Get LC predictions\n",
        "lc_true = Y_test[:, 1]\n",
        "lc_rule_pred = rule_pred[:, 1]\n",
        "lc_tree_pred = tree_pred[:, 1]\n",
        "\n",
        "models = ['Rule-based Classifier', 'Decision Tree Classifier']\n",
        "title = \"McNemar's table for LC class\"\n",
        "\n",
        "# Compute test\n",
        "lc_mcnemar = mcnemar_test(lc_true, lc_rule_pred, lc_tree_pred, model_names=models, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYaNqBZqxpTi"
      },
      "source": [
        "## Visualize tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0oYp5aAzM3B"
      },
      "source": [
        "# Export tree to DOT data\n",
        "dot_data = export_graphviz(dtree, out_file=None,\n",
        "                           feature_names=X_sel.columns,\n",
        "                           class_names=['GC', 'LC'],\n",
        "                           filled=True, proportion=True)\n",
        "\n",
        "# Draw and save graph\n",
        "graph = graphviz.Source(dot_data, format='PNG')\n",
        "graph.render(\"decision_tree_graph\")\n",
        "\n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}